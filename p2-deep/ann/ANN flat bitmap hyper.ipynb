{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 19,
			"metadata": {
				"cell_id": "05c94beced434dd7aa0e636f3b3931e8",
				"deepnote_cell_type": "code",
				"deepnote_to_be_reexecuted": false,
				"execution_millis": 1961,
				"execution_start": 1696507665262,
				"source_hash": "5ced2aa6"
			},
			"outputs": [],
			"source": [
				"import torch\n",
				"import torch.nn as nn\n",
				"import torch.nn.functional as F\n",
				"from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
				"import numpy as np\n",
				"import pandas as pd\n",
				"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
				"import matplotlib.pyplot as plt"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 20,
			"metadata": {
				"cell_id": "4c3bbaf2c7684b80a42ff6fdc24397cc",
				"deepnote_cell_type": "code",
				"deepnote_to_be_reexecuted": false,
				"execution_millis": 86,
				"execution_start": 1696507667163,
				"source_hash": "8f81bcab"
			},
			"outputs": [],
			"source": [
				"def determine_device():\n",
				"    device = \"cpu\"\n",
				"    if torch.cuda.is_available():\n",
				"        device = \"cuda\"\n",
				"    elif torch.backends.mps.is_available():\n",
				"        device = \"mps\" # Apple M1\n",
				"\n",
				"    print(f\"We are using device: {device}\")\n",
				"    return device"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 21,
			"metadata": {
				"cell_id": "5d3b4eec28c24363800a77261141743c",
				"deepnote_cell_type": "code",
				"deepnote_to_be_reexecuted": false,
				"execution_millis": 51,
				"execution_start": 1696507667169,
				"source_hash": "3f456c92"
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"We are using device: mps\n"
					]
				}
			],
			"source": [
				"DEVICE = determine_device()\n",
				"RANDOM_SEED = 24"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 22,
			"metadata": {
				"cell_id": "f9eca1ca252f4bd49635930ef33affc3",
				"deepnote_cell_type": "code",
				"deepnote_to_be_reexecuted": false,
				"execution_millis": 49,
				"execution_start": 1696507667209,
				"source_hash": "b623e53d"
			},
			"outputs": [],
			"source": [
				"torch.manual_seed(RANDOM_SEED)\n",
				"np.random.seed(RANDOM_SEED)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 23,
			"metadata": {},
			"outputs": [],
			"source": [
				"def get_data_item(path: str) -> pd.DataFrame:\n",
				"    df = pd.read_csv(path)\n",
				"    return df"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 24,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/html": [
							"<div>\n",
							"<style scoped>\n",
							"    .dataframe tbody tr th:only-of-type {\n",
							"        vertical-align: middle;\n",
							"    }\n",
							"\n",
							"    .dataframe tbody tr th {\n",
							"        vertical-align: top;\n",
							"    }\n",
							"\n",
							"    .dataframe thead th {\n",
							"        text-align: right;\n",
							"    }\n",
							"</style>\n",
							"<table border=\"1\" class=\"dataframe\">\n",
							"  <thead>\n",
							"    <tr style=\"text-align: right;\">\n",
							"      <th></th>\n",
							"      <th>uuid</th>\n",
							"      <th>angle</th>\n",
							"      <th>quality_average</th>\n",
							"      <th>difficulty_average</th>\n",
							"      <th>ascensionist_count</th>\n",
							"      <th>hold_0</th>\n",
							"      <th>hold_1</th>\n",
							"      <th>hold_2</th>\n",
							"      <th>hold_3</th>\n",
							"      <th>hold_4</th>\n",
							"      <th>...</th>\n",
							"      <th>hold_1692</th>\n",
							"      <th>hold_1693</th>\n",
							"      <th>hold_1694</th>\n",
							"      <th>hold_1695</th>\n",
							"      <th>hold_1696</th>\n",
							"      <th>hold_1697</th>\n",
							"      <th>hold_1698</th>\n",
							"      <th>hold_1699</th>\n",
							"      <th>hold_1700</th>\n",
							"      <th>hold_1701</th>\n",
							"    </tr>\n",
							"  </thead>\n",
							"  <tbody>\n",
							"    <tr>\n",
							"      <th>0</th>\n",
							"      <td>3a6b75ff3c4d4a4e83730daab1776861</td>\n",
							"      <td>0.571429</td>\n",
							"      <td>0.500000</td>\n",
							"      <td>23.0000</td>\n",
							"      <td>0.000000</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>...</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>1</th>\n",
							"      <td>9F2A5E33DD2449028B0487F4C9549DD7</td>\n",
							"      <td>0.857143</td>\n",
							"      <td>1.000000</td>\n",
							"      <td>21.5000</td>\n",
							"      <td>0.000042</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>...</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>2</th>\n",
							"      <td>31e90392f7a14b03a503bc910578958a</td>\n",
							"      <td>0.714286</td>\n",
							"      <td>0.939455</td>\n",
							"      <td>17.0056</td>\n",
							"      <td>0.010664</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>...</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>3</th>\n",
							"      <td>e37a79ea21704ffdba8a4add2968e878</td>\n",
							"      <td>0.642857</td>\n",
							"      <td>0.894735</td>\n",
							"      <td>25.9737</td>\n",
							"      <td>0.001547</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>...</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"    </tr>\n",
							"    <tr>\n",
							"      <th>4</th>\n",
							"      <td>B6A37D51FE384CFC96BBE6C2C26A6518</td>\n",
							"      <td>0.571429</td>\n",
							"      <td>1.000000</td>\n",
							"      <td>18.0000</td>\n",
							"      <td>0.000084</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>...</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"      <td>0.0</td>\n",
							"    </tr>\n",
							"  </tbody>\n",
							"</table>\n",
							"<p>5 rows Ã— 1707 columns</p>\n",
							"</div>"
						],
						"text/plain": [
							"                               uuid     angle  quality_average  \\\n",
							"0  3a6b75ff3c4d4a4e83730daab1776861  0.571429         0.500000   \n",
							"1  9F2A5E33DD2449028B0487F4C9549DD7  0.857143         1.000000   \n",
							"2  31e90392f7a14b03a503bc910578958a  0.714286         0.939455   \n",
							"3  e37a79ea21704ffdba8a4add2968e878  0.642857         0.894735   \n",
							"4  B6A37D51FE384CFC96BBE6C2C26A6518  0.571429         1.000000   \n",
							"\n",
							"   difficulty_average  ascensionist_count  hold_0  hold_1  hold_2  hold_3  \\\n",
							"0             23.0000            0.000000     0.0     0.0     0.0     0.0   \n",
							"1             21.5000            0.000042     0.0     0.0     0.0     0.0   \n",
							"2             17.0056            0.010664     0.0     0.0     0.0     0.0   \n",
							"3             25.9737            0.001547     0.0     0.0     0.0     0.0   \n",
							"4             18.0000            0.000084     0.0     0.0     0.0     0.0   \n",
							"\n",
							"   hold_4  ...  hold_1692  hold_1693  hold_1694  hold_1695  hold_1696  \\\n",
							"0     0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
							"1     0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
							"2     0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
							"3     0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
							"4     0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
							"\n",
							"   hold_1697  hold_1698  hold_1699  hold_1700  hold_1701  \n",
							"0        0.0        0.0        0.0        0.0        0.0  \n",
							"1        0.0        0.0        0.0        0.0        0.0  \n",
							"2        0.0        0.0        0.0        0.0        0.0  \n",
							"3        0.0        0.0        0.0        0.0        0.0  \n",
							"4        0.0        0.0        0.0        0.0        0.0  \n",
							"\n",
							"[5 rows x 1707 columns]"
						]
					},
					"execution_count": 24,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"train_data = get_data_item('../data/data_meta_holds_train_norm.csv')\n",
				"train_data.head()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 25,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"(torch.Size([93363]), tensor(0.))"
						]
					},
					"execution_count": 25,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"ascentions_per_route = train_data['ascensionist_count'].values\n",
				"del train_data\n",
				"\n",
				"# sample_weights = (ascentions_per_route / sum(ascentions_per_route)) * len(ascentions_per_route)\n",
				"sample_weights = ascentions_per_route * len(ascentions_per_route) # Because we already normalized the data\n",
				"sample_weights_tensor = torch.Tensor(sample_weights)\n",
				"\n",
				"sample_weights_tensor.shape, sample_weights_tensor[0]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 26,
			"metadata": {},
			"outputs": [],
			"source": [
				"class BoulderingDataset(Dataset):\n",
				"    def __init__(self, *, train_path='../data/data_meta_holds_train_norm.csv', X_transform=None, Y_transform=None):\n",
				"        super().__init__()\n",
				"        train_data = get_data_item(train_path)\n",
				"        Y_column_name = 'difficulty_average'\n",
				"        ID_column_name = 'uuid'\n",
				"        self.X = torch.Tensor(train_data.drop([Y_column_name, ID_column_name], axis=1).values)\n",
				"        self.Y = torch.Tensor(train_data[Y_column_name].values).unsqueeze(1)\n",
				"        self.IDs = train_data[ID_column_name].values\n",
				"        self.X_transform = X_transform\n",
				"        self.Y_transform = Y_transform\n",
				"\n",
				"    def __getitem__(self, index):\n",
				"        x = self.X[index]\n",
				"        y = self.Y[index]\n",
				"\n",
				"        if self.X_transform:\n",
				"            x = self.X_transform(x)\n",
				"        if self.Y_transform:\n",
				"            y = self.Y_transform(y)\n",
				"\n",
				"        return x, y, self.IDs[index]\n",
				"\n",
				"    def __len__(self):\n",
				"        return len(self.X)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 27,
			"metadata": {},
			"outputs": [],
			"source": [
				"class ANN(nn.Module):\n",
				"    def __init__(self, input_dim: int, hidden_dims: list, output_dim: int, activation_fn=nn.LeakyReLU):\n",
				"        super().__init__()\n",
				"        layers = []\n",
				"        layers.append(nn.Linear(input_dim, hidden_dims[0]))\n",
				"        layers.append(activation_fn())\n",
				"        \n",
				"        for i in range(1, len(hidden_dims)):\n",
				"            layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
				"            layers.append(activation_fn())\n",
				"        \n",
				"        self.layers = nn.ModuleList(layers)\n",
				"        self.fcOut = nn.Linear(hidden_dims[-1], output_dim)\n",
				"\n",
				"    def forward(self, x) -> torch.Tensor:\n",
				"        for layer in self.layers:\n",
				"            x = layer(x)\n",
				"        return self.fcOut(x)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 28,
			"metadata": {},
			"outputs": [],
			"source": [
				"def compute_regression_metrics(y_pred, y_true, *, metrics=['MSE', 'MAE', 'R2'], mse=None):\n",
				"    metrics_dict = {}\n",
				"    \n",
				"    if 'MSE' in metrics or 'RMSE' in metrics:\n",
				"        if mse:\n",
				"            metrics_dict['MSE'] = mse\n",
				"        else:\n",
				"            metrics_dict['MSE'] = mean_squared_error(y_pred, y_true)\n",
				"\n",
				"    if 'RMSE' in metrics:\n",
				"        metrics_dict['RMSE'] = np.sqrt(metrics_dict['MSE'])\n",
				"            \n",
				"    if 'MAE' in metrics:\n",
				"        metrics_dict['MAE'] = mean_absolute_error(y_pred, y_true)\n",
				"        \n",
				"    if 'R2' in metrics:\n",
				"        metrics_dict['R2'] = r2_score(y_pred, y_true)\n",
				"        \n",
				"    return metrics_dict"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 29,
			"metadata": {},
			"outputs": [],
			"source": [
				"def train_one_epoch(model, train_loader, optimizer, loss_fn, device=DEVICE):\n",
				"    model.train()\n",
				"    \n",
				"    total_loss = 0.0\n",
				"    for features, targets, id in train_loader:\n",
				"        features, targets = features.to(device), targets.to(device)\n",
				"        \n",
				"        logits = model(features)\n",
				"        loss = loss_fn(logits, targets)\n",
				"\n",
				"        optimizer.zero_grad()\n",
				"        loss.backward()\n",
				"        optimizer.step()\n",
				"        \n",
				"        total_loss += loss.item()\n",
				"    \n",
				"    return total_loss / len(train_loader)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 30,
			"metadata": {},
			"outputs": [],
			"source": [
				"def evaluate(model, valid_loader, *, device=DEVICE, mse=None):\n",
				"    model.eval()\n",
				"    \n",
				"    with torch.no_grad():\n",
				"        y_pred, y_true = [], []\n",
				"        for features, targets, id in valid_loader:\n",
				"            features = features.to(device)\n",
				"            logits = model(features).cpu().numpy()\n",
				"            y_pred.extend(logits)\n",
				"            y_true.extend(targets.cpu().numpy())\n",
				"    \n",
				"    metrics = compute_regression_metrics(np.array(y_pred), np.array(y_true), mse=mse)\n",
				"    return metrics"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 31,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"93363"
						]
					},
					"execution_count": 31,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"dataset = BoulderingDataset()\n",
				"\n",
				"len(dataset)"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 32,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"(torch.Size([93363, 1]), torch.Size([93363, 1705]))"
						]
					},
					"execution_count": 32,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"dataset.Y.shape, dataset.X.shape"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 33,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"(tensor([[5.7143e-01, 5.0000e-01, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
							"          0.0000e+00],\n",
							"         [8.5714e-01, 1.0000e+00, 4.1818e-05,  ..., 0.0000e+00, 0.0000e+00,\n",
							"          0.0000e+00]]),\n",
							" tensor([[23.0000],\n",
							"         [21.5000]]))"
						]
					},
					"execution_count": 33,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"dataset.X[:2], dataset.Y[:2]"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 34,
			"metadata": {},
			"outputs": [],
			"source": [
				"# Source https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets\n",
				"\n",
				"from torch.utils.data import Subset\n",
				"from sklearn.model_selection import train_test_split\n",
				"\n",
				"TEST_SIZE = 0.3\n",
				"train_indices, test_indices, _, _ = train_test_split(\n",
				"    range(len(dataset.Y)),\n",
				"    dataset.Y,\n",
				"    test_size=TEST_SIZE,\n",
				"    random_state=RANDOM_SEED,\n",
				"    shuffle=True\n",
				")\n",
				"\n",
				"PARTIAL_DATASET = False\n",
				"PARTIAL_PCT = 0.05\n",
				"if PARTIAL_DATASET:\n",
				"    train_indices = train_indices[:int(len(train_indices) * PARTIAL_PCT)]\n",
				"    test_indices = test_indices[:int(len(test_indices) * PARTIAL_PCT)]\n",
				"\n",
				"train_dataset = Subset(dataset, train_indices)\n",
				"valid_dataset = Subset(dataset, test_indices)\n",
				"del dataset\n",
				"if DEVICE == \"cuda\":\n",
				"    torch.cuda.empty_cache()\n",
				"\n",
				"train_sample_weights_tensor = sample_weights_tensor[train_indices]\n",
				"sampler = WeightedRandomSampler(train_sample_weights_tensor, len(train_sample_weights_tensor))"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 35,
			"metadata": {},
			"outputs": [
				{
					"data": {
						"text/plain": [
							"(1705, 1)"
						]
					},
					"execution_count": 35,
					"metadata": {},
					"output_type": "execute_result"
				}
			],
			"source": [
				"input_dim = train_dataset.dataset.X.shape[1] # columns of the flattened bitmap + metadata (angle, quality, ascents)\n",
				"ouput_dim = 1 # regression\n",
				"\n",
				"input_dim, ouput_dim"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 36,
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 00:02:36,996] A new study created in memory with name: no-name-ba2e379a-5fcf-40db-b8ad-807be825b83f\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Random seed: 24, train size: 65354, valid size: 28009. Device: mps\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 01:09:41,987] Trial 0 finished with value: 1.5725665092468262 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 76\n",
						"Saving first weights with MSE 4.09\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 04:31:45,811] Trial 1 finished with value: 1.6831867694854736 and parameters: {'BATCH_SIZE': 4, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 25\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 08:08:32,055] Trial 2 finished with value: 1.6446332931518555 and parameters: {'BATCH_SIZE': 8, 'LEARNING_RATE': 0.0005, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Gradient explosion at epoch 44. Stopping training. train_loss: 2.7444523677833903\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 10:10:18,022] Trial 3 finished with value: 1.7151546478271484 and parameters: {'BATCH_SIZE': 4, 'LEARNING_RATE': 0.005, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 30\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 10:59:56,447] Trial 4 finished with value: 1.6054675579071045 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 60\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 11:21:29,798] Trial 5 finished with value: 1.8552310466766357 and parameters: {'BATCH_SIZE': 8, 'LEARNING_RATE': 0.0005, 'SCHEDULER_FACTOR': 0.3, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Gradient explosion at epoch 11. Stopping training. train_loss: 30.507381767917177\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 14:09:42,593] Trial 6 finished with value: 1.5989214181900024 and parameters: {'BATCH_SIZE': 4, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 62\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 15:48:00,582] Trial 7 finished with value: 1.585307240486145 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0005, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 123\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 17:25:15,911] Trial 8 finished with value: 1.6431796550750732 and parameters: {'BATCH_SIZE': 4, 'LEARNING_RATE': 0.0005, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 31\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 18:13:53,103] Trial 9 finished with value: 1.6066139936447144 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 56\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 18:20:38,659] Trial 10 finished with value: 1.8747860193252563 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.001, 'SCHEDULER_FACTOR': 0.3, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Gradient explosion at epoch 8. Stopping training. train_loss: 9.369982703974884\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 18:23:14,420] Trial 11 finished with value: 2.117952823638916 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.005, 'SCHEDULER_FACTOR': 0.3, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Gradient explosion at epoch 3. Stopping training. train_loss: 3905644127.771279\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 18:57:43,372] Trial 12 finished with value: 1.6490471363067627 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0005, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 40\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 19:46:18,275] Trial 13 finished with value: 1.6049894094467163 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 56\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 20:21:24,328] Trial 14 finished with value: 1.634039282798767 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0005, 'SCHEDULER_FACTOR': 0.3, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 46\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 21:07:19,560] Trial 15 finished with value: 1.5965079069137573 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 57\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 21:12:10,836] Trial 16 finished with value: 3.61995792388916 and parameters: {'BATCH_SIZE': 8, 'LEARNING_RATE': 0.005, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Gradient explosion at epoch 3. Stopping training. train_loss: 3338.094598407718\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 21:49:49,968] Trial 17 finished with value: 1.6297515630722046 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 48\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 22:04:21,934] Trial 18 finished with value: 1.6938062906265259 and parameters: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0005, 'SCHEDULER_FACTOR': 0.3, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.PReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Gradient explosion at epoch 15. Stopping training. train_loss: 17.925700521311807\n"
					]
				},
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"[I 2023-10-25 23:08:25,241] Trial 19 finished with value: 1.6315791606903076 and parameters: {'BATCH_SIZE': 8, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1500, 1100, 700, 300', 'ACTIVATION_FN': 'nn.ReLU'}. Best is trial 0 with value: 1.5725665092468262.\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Early stopping at epoch 44\n",
						"Finished study with 20 trials. Best MAE valid: 1.57, best params: {'BATCH_SIZE': 16, 'LEARNING_RATE': 0.0001, 'SCHEDULER_FACTOR': 0.1, 'SCHEDULER_PATIENCE': 5, 'HIDDEN_DIMS': '1250, 1000, 750, 500, 250', 'ACTIVATION_FN': 'nn.PReLU'}\n",
						"Saved results to results.csv\n"
					]
				}
			],
			"source": [
				"import optuna\n",
				"\n",
				"DEBUG = False\n",
				"if PARTIAL_DATASET:\n",
				"    print(f'ATTENTION: Using only {PARTIAL_PCT * 100}% of the dataset!')\n",
				"\n",
				"print(f'Random seed: {RANDOM_SEED}, train size: {len(train_dataset)}, valid size: {len(valid_dataset)}. Device: {DEVICE}')\n",
				"\n",
				"results = []\n",
				"\n",
				"def objective(trial):\n",
				"    best_valid_mse = float('inf')\n",
				"    best_model_state = None\n",
				"    gradient_exploded = False\n",
				"    metrics = {\n",
				"        'MSE': { 'train': [], 'valid': [] },\n",
				"        'MAE': { 'train': [], 'valid': [] },\n",
				"        'R2': { 'train': [], 'valid': [] }\n",
				"    }\n",
				"    MAX_EPOCHS = 200\n",
				"\n",
				"    BATCH_SIZE = trial.suggest_categorical('BATCH_SIZE', [4, 8, 16])\n",
				"    LEARNING_RATE = trial.suggest_categorical('LEARNING_RATE', [0.0001, 0.0005, 0.001, 0.005])\n",
				"    SCHEDULER_FACTOR = trial.suggest_categorical('SCHEDULER_FACTOR', [0.1, 0.3])\n",
				"    SCHEDULER_PATIENCE = trial.suggest_categorical('SCHEDULER_PATIENCE', [5])\n",
				"    HIDDEN_DIMS = trial.suggest_categorical('HIDDEN_DIMS', [\n",
				"        '1250, 1000, 750, 500, 250',\n",
				"        '1500, 1100, 700, 300',\n",
				"    ])\n",
				"    HIDDEN_DIMS = tuple(map(int, HIDDEN_DIMS.split(', ')))\n",
				"    ACTIVATION_FN = trial.suggest_categorical('ACTIVATION_FN', ['nn.PReLU', 'nn.ReLU'])\n",
				"    ACTIVATION_FN = eval(ACTIVATION_FN)\n",
				"\n",
				"    model = ANN(input_dim, HIDDEN_DIMS, ouput_dim, activation_fn=ACTIVATION_FN).to(DEVICE)\n",
				"\n",
				"    DEBUG and print(f\"Model layers: {[input_dim, *HIDDEN_DIMS, ouput_dim]}\")\n",
				"    DEBUG and print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')\n",
				"    DEBUG and print(f'Trial {trial.number} params: {trial.params}')\n",
				"\n",
				"    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
				"    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=SCHEDULER_PATIENCE, factor=SCHEDULER_FACTOR, verbose=True if DEBUG else False)\n",
				"    loss_fn = nn.MSELoss()\n",
				"    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
				"    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
				"\n",
				"    for epoch in range(MAX_EPOCHS):\n",
				"        gradient_exploded = False\n",
				"        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
				"\n",
				"        train_metrics = evaluate(model, train_loader, mse=train_loss)\n",
				"        valid_metrics = evaluate(model, valid_loader)\n",
				"\n",
				"        if epoch > 0 and train_loss > 1000 * metrics['MSE']['train'][-1]:\n",
				"            print(f'Gradient explosion at epoch {epoch + 1}. Exiting epoch. train_loss: {train_loss}')\n",
				"            gradient_exploded = True\n",
				"            break\n",
				"\n",
				"        if valid_metrics['MSE'] < best_valid_mse:\n",
				"            DEBUG and print(f'New best weights at epoch {epoch + 1}.')\n",
				"            best_valid_mse = valid_metrics['MSE']\n",
				"            best_model_state = model.state_dict()\n",
				"\n",
				"        scheduler.step(valid_metrics['MSE'])\n",
				"        \n",
				"        DEBUG and print(f'Epoch {epoch + 1}/{MAX_EPOCHS}, lr: {optimizer.param_groups[0][\"lr\"]:.0e}; train MAE {train_metrics[\"MAE\"]:.2f}, R2 {train_metrics[\"R2\"]:.2f}, MSE {train_metrics[\"MSE\"]:.2f}; valid MAE {valid_metrics[\"MAE\"]:.2f}, R2 {valid_metrics[\"R2\"]:.2f}, MSE {valid_metrics[\"MSE\"]:.2f}')\n",
				"        for metric in train_metrics.keys():\n",
				"            metrics[metric]['train'].append(train_metrics[metric])\n",
				"            metrics[metric]['valid'].append(valid_metrics[metric])\n",
				"\n",
				"        # Early stopping if MSE of the last 3 epochs is higher than the mean of the 15 previous epochs\n",
				"        if epoch > 17 and np.mean(metrics['MSE']['valid'][-3:]) >= np.mean(metrics['MSE']['valid'][-18:-3]):\n",
				"            print(f'Early stopping at epoch {epoch + 1}')\n",
				"            break\n",
				"\n",
				"        if epoch == MAX_EPOCHS - 1:\n",
				"            print(f'Wow, max epochs reached ({MAX_EPOCHS}), stopping.')\n",
				"\n",
				"    if trial.number == 0 and not gradient_exploded:\n",
				"        print(f'Saving first weights with MSE {best_valid_mse:.2f}')\n",
				"        torch.save(best_model_state, 'best_model_state.pth')\n",
				"    elif best_valid_mse < study.best_value:\n",
				"        print(f'New overall best MSE in trial {trial.number}: {best_valid_mse:.2f}, saving weights.')\n",
				"        torch.save(best_model_state, f'best_model_state-trial-{trial.number}.pth')\n",
				"\n",
				"    valid_mae_mean = np.mean(metrics['MAE']['valid'])\n",
				"\n",
				"    results.append({\n",
				"        'BATCH_SIZE': BATCH_SIZE,\n",
				"        'LEARNING_RATE': LEARNING_RATE,\n",
				"        'SCHEDULER_FACTOR': SCHEDULER_FACTOR,\n",
				"        'SCHEDULER_PATIENCE': SCHEDULER_PATIENCE,\n",
				"        'HIDDEN_DIMS': HIDDEN_DIMS,\n",
				"        'ACTIVATION_FN': ACTIVATION_FN.__name__,\n",
				"        'MSE_train': np.mean(metrics['MSE']['train']),\n",
				"        'MSE_valid': np.mean(metrics['MSE']['valid']),\n",
				"        'MAE_train': np.mean(metrics['MAE']['train']),\n",
				"        'MAE_valid': valid_mae_mean,\n",
				"        'R2_train': np.mean(metrics['R2']['train']),\n",
				"        'R2_valid': np.mean(metrics['R2']['valid']),\n",
				"    })\n",
				"\n",
				"    return valid_mae_mean\n",
				"\n",
				"study = optuna.create_study(direction='minimize')\n",
				"study.optimize(objective, n_trials=20)\n",
				"\n",
				"print(f'Finished study with {len(study.trials)} trials. Best MAE valid: {study.best_value:.2f}, best params: {study.best_params}')\n",
				"\n",
				"df = pd.DataFrame(results)\n",
				"df.to_csv('results.csv', index=False)\n",
				"print('Saved results to results.csv')"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 37,
			"metadata": {},
			"outputs": [],
			"source": [
				"def plot_metric_per_epoch(metric: str, *, train_metrics: list, valid_metrics: list):\n",
				"    plt.figure(figsize=(10, 6))\n",
				"    plt.plot(train_metrics, label=f'train {metric}')\n",
				"    plt.plot(valid_metrics, label=f'valid {metric}')\n",
				"    plt.xlabel('Epoch')\n",
				"    plt.ylabel(metric)\n",
				"    plt.legend()\n",
				"    plt.show()"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {},
			"outputs": [],
			"source": [
				"plot_metric_per_epoch('MSE', train_metrics=study.best_params['MSE_train'], valid_metrics=study.best_params['MSE_valid'])\n",
				"plot_metric_per_epoch('MAE', train_metrics=study.best_params['MAE_train'], valid_metrics=study.best_params['MAE_valid'])\n",
				"plot_metric_per_epoch('R2', train_metrics=study.best_params['R2_train'], valid_metrics=study.best_params['R2_valid'])"
			]
		}
	],
	"metadata": {
		"deepnote": {},
		"deepnote_execution_queue": [],
		"deepnote_notebook_id": "569901ed3e294625bf139cfcf28760dd",
		"kernelspec": {
			"display_name": "torch-3-10",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.10.12"
		},
		"orig_nbformat": 2
	},
	"nbformat": 4,
	"nbformat_minor": 0
}
